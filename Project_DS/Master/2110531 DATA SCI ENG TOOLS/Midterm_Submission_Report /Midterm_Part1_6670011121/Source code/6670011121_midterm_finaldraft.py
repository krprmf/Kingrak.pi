# -*- coding: utf-8 -*-
"""6670011121_midterm_FinalDraft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hYF9rkWc-5Gz_QuTS-uK5lk6JVFM2TX2

https://colab.research.google.com/drive/1hYF9rkWc-5Gz_QuTS-uK5lk6JVFM2TX2?usp=sharing

# Mount drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Pip install necessery library"""

!pip install transformers

pip install datasets

pip install transformers[torch]

!pip install wandb

!wandb login

"""# Import necessery library"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import numpy as np
# %matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import transformers
import wandb

import re
import nltk
from nltk.corpus import stopwords

from sklearn.preprocessing import MultiLabelBinarizer
from datasets import DatasetDict, Dataset
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from transformers import EvalPrediction

name = "test_run_best"

wandb.init(project="midterm_ds", name=name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

data_train = pd.read_json('/content/drive/MyDrive/2110531-data-science-2023-01/train_for_student.json')
data_train = data_train.T
data_train.drop_duplicates(subset=['Title','Abstract'], inplace=True)
data_train.head()

data_train['Abstract'].iloc[0]

data_test = pd.read_json('/content/drive/MyDrive/2110531-data-science-2023-01/test_for_student.json')

data_test = data_test.T
data_test.head()

data_train.info()

print(len(data_train),len(data_test))
print(len(data_train)/len(data_test),':',len(data_test)/len(data_test))

"""# Data preprocessing"""

df_test = data_test.copy()
cols = df_test.columns.tolist()
df_test['Text'] = df_test['Title']+". "+df_test['Abstract']
df_test = df_test.drop(columns=['Title', 'Abstract'])
# df.reset_index(drop=True, inplace=True)
df_test

df = data_train.copy()
cols = df.columns.tolist()
df['Text'] = df['Title']+". "+df['Abstract']
df['labels'] = df['Classes']
df = df.drop(columns=['Title', 'Abstract','Classes'])
df.reset_index(drop=True, inplace=True)
df

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])
re_stop_words = re.compile(r"\b(" + "|".join(stop_words) + ")\\W", re.I)

def removeStopWords(sentence):
    global re_stop_words
    return re_stop_words.sub(" ", sentence)

df['Text'] = df['Text'].apply(removeStopWords)
df_test['Text'] = df_test['Text'].apply(removeStopWords)

def text_preprocessing(text):

    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"won't", "will not ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub(r"\'\n", " ", text)
    text = re.sub(r"-", " ", text)
    text = re.sub(r"\'\xa0", " ", text)
    text = re.sub('\s+', ' ', text)
    text = ''.join(c for c in text if not c.isnumeric())
    text = re.sub(r'(@.*?)[\s]', ' ', text)
    text = re.sub(r'&amp;', '&', text)
    text = re.sub(r'\s+', ' ', text).strip()

    return text

df['Text'] = df['Text'].apply(text_preprocessing)
df_test['Text'] = df_test['Text'].apply(text_preprocessing)

df

labels = df['labels']
mlb = MultiLabelBinarizer()
one_hot_labels = mlb.fit_transform(labels)

encoded_labels_df = pd.DataFrame(one_hot_labels, columns=mlb.classes_)
encoded_labels_df = encoded_labels_df.astype(int)
encoded_labels_df.head()

df = pd.concat([df['Text'], encoded_labels_df], axis=1)
df[list(mlb.classes_)] = df[list(mlb.classes_)].astype(int)
df.head()

cat = list(mlb.classes_)

# Find rows where all values are 0
rows_all_zero = df.loc[(df.iloc[:, 1:] == 0).all(axis=1)]
print(f"Deleteable rows:{rows_all_zero.shape[0]}")

df_n = df.copy()

cols = df.columns.tolist()
labels = cols[1:]
num_labels = len(labels)
id2label = {id:label for id, label in enumerate(labels)}
label2id = {label:id for id, label in enumerate(labels)}
print(label2id)
print(id2label)

df['one_hot_labels'] = df[labels].values.tolist()
df.head()

df2 = df.copy()
df2.drop(inplace=True,columns=labels)
df2

"""# Model

## Split and Tokenize
"""

df_train, df_val = train_test_split(df_n, random_state=32, test_size=0.2, shuffle=True)

train_dataset = Dataset.from_pandas(df_train,preserve_index=False)
val_dataset = Dataset.from_pandas(df_val,preserve_index=False)

dataset_dict = DatasetDict({"train": train_dataset, "val": val_dataset})
print(dataset_dict)

example = dataset_dict['train'][0]
example

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
max_length = 128

def tokenize_data(data):
  text = data["Text"]
  encode = tokenizer(text, padding="max_length", truncation=True, max_length=max_length)
  labels_batch = {k: data[k] for k in data.keys() if k in labels}
  labels_matrix = np.zeros((len(text), len(labels)))
  for id, label in enumerate(labels):
    labels_matrix[:, id] = labels_batch[label]
  encode["labels"] = labels_matrix.tolist()
  return encode

encoded_dataset = dataset_dict.map(tokenize_data, batched=True, remove_columns=dataset_dict['train'].column_names)

example = encoded_dataset['train'][0]
print(example.keys())

print(tokenizer.decode(example['input_ids']))
print(example['labels'])

[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]

encoded_dataset.set_format("torch")

"""## Set parameter of model"""

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased",
                                                           problem_type="multi_label_classification",
                                                           num_labels=18,
                                                           id2label=id2label,
                                                           label2id=label2id)

# Best param
batch_size = 8
learning_rate = 1e-4
num_train_epochs = 35
weight_decay = 1e-4

training_args = TrainingArguments(
    f"bert-finetuned-sem_eval-english",
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=learning_rate,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_train_epochs,
    weight_decay=weight_decay,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to="wandb"
)

"""## Define eval matrics
source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/
"""

from sklearn.metrics import f1_score, roc_auc_score, accuracy_score
from transformers import EvalPrediction
import torch

def multi_label_metrics(predictions, labels):
    sigmoid = torch.nn.Sigmoid()
    probs = sigmoid(torch.Tensor(predictions))
    y_pred = np.zeros(probs.shape)
    y_pred[np.where(probs >= 0.5)] = 1
    y_true = labels
    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro')
    roc_auc = roc_auc_score(y_true, y_pred, average = 'macro')
    accuracy = accuracy_score(y_true, y_pred)
    metrics = {'f1': f1_macro_average,
               'roc_auc': roc_auc,
               'accuracy': accuracy}
    return metrics

def compute_metrics(p: EvalPrediction):
    preds = p.predictions[0] if isinstance(p.predictions,
            tuple) else p.predictions
    result = multi_label_metrics(
        predictions=preds,
        labels=p.label_ids)
    return result

encoded_dataset['train']['input_ids'][0]

#forward pass
outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))
outputs

trainer = Trainer(
    model,
    training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["val"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

"""# Train ans valtidate model"""

trainer.train()

trainer.evaluate()

"""#Test and save"""

trainer.save_model("/content/drive/MyDrive/2110531-data-science-2023-01/best1")

checkpoint = "/content/drive/MyDrive/2110531-data-science-2023-01/best1"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint,
                                                           problem_type="multi_label_classification",
                                                           num_labels=len(labels),
                                                           id2label=id2label,
                                                           label2id=label2id)

model.to(device)

predictions = []
for i in df_test['Text']:
  encoding = tokenizer(i, return_tensors="pt")
  encoding = {k: v.to(trainer.model.device) for k,v in encoding.items()}
  outputs = trainer.model(**encoding)
  logits = outputs.logits
  sigmoid = torch.nn.Sigmoid()
  probs = sigmoid(logits.squeeze().cpu())
  prediction = np.zeros(probs.shape)
  prediction[np.where(probs >= 0.5)] = 1
  predictions.append(np.array(prediction))

len(predictions)

outputs = np.array(predictions)

outputs

outputs[0,:]

pred = {}
for i in range(len(outputs)):
  for j in range(len(labels)):
    if labels[j] not in pred:
      pred[labels[j]] = []
      pred[labels[j]].append(outputs[i,j])
      print(pred)
    else:
      pred[labels[j]].append(outputs[i,j])

new_labels = ['CE','ENV','BME','PE','METAL','ME','EE','CPE','OPTIC','NANO','CHE','MATENG','AGRI','EDU','IE','SAFETY','MATH','MATSCI']

pred = {}
for i in range(len(outputs)):
  for j in range(len(labels)):
    if labels[j] not in pred:
      pred[labels[j]] = []
      pred[labels[j]].append(outputs[i,j])
    else:
      pred[labels[j]].append(outputs[i,j])

new_preds = {key: pred[key] for key in new_labels}
print(new_preds)

data_test

data_test.index

bf_df = data_test.copy()
df_pred= pd.DataFrame.from_dict(new_preds)
df_pred.set_index(data_test.index, inplace=True)
df_pred.reset_index(inplace = True)
df_pred.rename(columns = {'index':'id'}, inplace = True)
# df_pred['id'] = df_pred['index']

df_pred[labels] = df_pred[labels].astype(int)
df_pred

df_pred.to_csv('/content/drive/MyDrive/2110531-data-science-2023-01/predict.csv', index=False)

df_verify = pd.read_csv('/content/drive/MyDrive/2110531-data-science-2023-01/predict.csv')
print(df_verify)